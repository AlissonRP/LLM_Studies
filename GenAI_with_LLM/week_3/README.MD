## Reinforcement Learning from Human Feedback (RLHF)
* Individualização do modelo
* *Reward Model*: Guarda todas as preferências do usuário aprendidas ao longo do HF
* A gente pode fazer um *fine-tuning* utilizando o *reward model*